# -*- coding: utf-8 -*-
"""
RAG ä¸­æ–‡é—®ç­”è„šæ‰‹æ¶ - å•æ–‡ä»¶
- æ”¯æŒ TXT/PDF/DOCX/MD åŠ è½½
- ä¸­æ–‡å‹å¥½åˆ†å—
- Chroma å‘é‡åº“æŒä¹…åŒ–ï¼ˆå«å¢é‡ï¼‰
- BGE ä¸­æ–‡ Embeddingï¼ˆä¿®å¤ client å±æ€§ç¼ºå¤±ï¼‰
- DeepSeek 7B Chat æ¨ç†ï¼ˆ4bit ä¼˜å…ˆï¼Œå¤±è´¥è‡ªåŠ¨é™çº§ï¼‰
- æ£€ç´¢é˜ˆå€¼è¿‡æ»¤ +ï¼ˆå¯é€‰ï¼‰MMR å»å†—
- æ›´ç¨³å¥çš„æ¨¡å‹ç¼“å­˜ä¸é•œåƒé…ç½®
"""

import os
import sys
import torch
import hashlib
from pathlib import Path

# ========================== 0. ç¯å¢ƒä¸é•œåƒ ==========================
# æœ¬åœ°ç¼“å­˜åˆ° ./modelsï¼Œé¿å…æ•£è½ç”¨æˆ·ç›®å½•
MODEL_BASE_DIR = Path("models")
MODEL_BASE_DIR.mkdir(exist_ok=True)
os.environ.setdefault("HF_HOME", str(MODEL_BASE_DIR.resolve()))
os.environ.setdefault("TRANSFORMERS_CACHE", str(MODEL_BASE_DIR.resolve()))
# æ¸…åé•œåƒï¼ˆå¦‚é‡å¼‚å¸¸ä¼šæç¤ºå›é€€å®˜æ–¹æºï¼‰
# HF_MIRROR = "https://mirrors.tuna.tsinghua.edu.cn/hugging-face-models"
# os.environ.setdefault("HF_ENDPOINT", HF_MIRROR)

# ========================== 1. åŸºç¡€é…ç½® ==========================
DOCS_DIR = "knowledge_base"          # å¾…å¤„ç†æ–‡æ¡£ç›®å½•
VECTOR_DB_DIR = "chroma_vector_db"   # å‘é‡åº“æŒä¹…åŒ–ç›®å½•

# æ¨¡å‹ï¼ˆ7B æ˜¾å­˜â‰¥8GBï¼Œ4bit åçº¦ 5~6GBï¼›CPU å»ºè®®ç”¨ 3B æ¨¡å‹å¦‚ deepseek-ai/deepseek-llm-3b-chatï¼‰
EMBEDDING_MODEL_NAME = "BAAI/bge-large-zh-v1.5"  # ä¸­æ–‡ Embedding æœ€ä¼˜é€‰æ‹©ä¹‹ä¸€
LLM_MODEL_NAME = "deepseek-ai/deepseek-llm-7b-chat"

EMBEDDING_MODEL_PATH = MODEL_BASE_DIR / EMBEDDING_MODEL_NAME
LLM_MODEL_PATH = MODEL_BASE_DIR / LLM_MODEL_NAME

# æ£€ç´¢å‚æ•°ï¼ˆä¸­æ–‡åœºæ™¯ç»éªŒå€¼ï¼‰
TOP_K = 5                   # å¬å› top N ç›¸å…³ç‰‡æ®µ
CHUNK_SIZE = 800            # ä¸­æ–‡åˆ†å—é•¿åº¦ï¼ˆå•å—çº¦ 2400 å­—ç¬¦ï¼ŒåŒ¹é… BGE æœ€ä¼˜è¾“å…¥ï¼‰
CHUNK_OVERLAP = 100         # åˆ†å—é‡å é•¿åº¦ï¼ˆé¿å…è¯­ä¹‰æ–­è£‚ï¼‰
DISTANCE_THRESHOLD = 0.35   # ä½™å¼¦è·ç¦»é˜ˆå€¼ï¼ˆè¶Šå°è¶Šç›¸ä¼¼ï¼Œä¸­æ–‡å»ºè®® 0.3~0.4ï¼‰
USE_MMR = False             # å¯ç”¨ MMR å»å†—ï¼ˆéœ€æ›´å¤šè®¡ç®—ï¼Œå°æ–‡æ¡£å¯å…³é—­ï¼‰

# ========================== 2. æ–‡æ¡£åŠ è½½ ==========================
def load_documents():
    from langchain_community.document_loaders import (
        DirectoryLoader, TextLoader, Docx2txtLoader,
        UnstructuredMarkdownLoader, PyPDFium2Loader
    )
    loaders = [
        # TXT æ–‡æ¡£åŠ è½½
        DirectoryLoader(
            DOCS_DIR, glob="**/*.txt",
            loader_cls=TextLoader, loader_kwargs={"encoding": "utf-8"},
            show_progress=True
        ),
        # PDF æ–‡æ¡£åŠ è½½ï¼ˆPyPDFium2 æ¯” PyPDF æ›´ç²¾å‡†ï¼Œæ”¯æŒä¸­æ–‡ï¼‰
        DirectoryLoader(
            DOCS_DIR, glob="**/*.pdf",
            loader_cls=PyPDFium2Loader, show_progress=True
        ),
        # DOCX æ–‡æ¡£åŠ è½½
        DirectoryLoader(
            DOCS_DIR, glob="**/*.docx",
            loader_cls=Docx2txtLoader, show_progress=True
        ),
        # Markdown æ–‡æ¡£åŠ è½½ï¼ˆè¡¥å…… mode å‚æ•°ï¼Œä¿®å¤åŠ è½½å¤±è´¥ï¼‰
        DirectoryLoader(
            DOCS_DIR, glob="**/*.md",
            loader_cls=UnstructuredMarkdownLoader,
            loader_kwargs={"encoding": "utf-8", "mode": "single"},  # å•æ®µè½æ¨¡å¼ï¼Œé¿å…æ‹†åˆ†æ··ä¹±
            show_progress=True
        ),
    ]

    documents = []
    for loader in loaders:
        try:
            docs = loader.load()
            for doc in docs:
                doc.metadata["file_type"] = loader.loader_cls.__name__
                # è¡¥å……æ–‡æ¡£è·¯å¾„ç®€åŒ–ï¼ˆä»…ä¿ç•™æ–‡ä»¶åï¼Œé¿å…è·¯å¾„è¿‡é•¿ï¼‰
                doc.metadata["source"] = Path(doc.metadata["source"]).name
            print(f"âœ… æˆåŠŸåŠ è½½ {len(docs)} ä¸ª {loader.loader_cls.__name__} æ–‡æ¡£")
            documents.extend(docs)
        except Exception as e:
            error_msg = str(e)[:120] + "..." if len(str(e)) > 120 else str(e)
            print(f"âŒ {loader.loader_cls.__name__} åŠ è½½å¤±è´¥: {error_msg}")
    print(f"\nğŸ“Š æ€»è®¡æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
    return documents

# ========================== 3. ä¸­æ–‡å‹å¥½åˆ†å—ï¼ˆä¼˜åŒ–è¯­ä¹‰è¿ç»­æ€§ï¼‰ ==========================
def split_documents(documents):
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    # ä¸­æ–‡ä¸“å±åˆ†éš”ç¬¦ï¼ˆä¼˜å…ˆæŒ‰å¥æ‹†åˆ†ï¼Œå…¶æ¬¡æŒ‰æ ‡ç‚¹ï¼Œæœ€åæŒ‰æ¢è¡Œï¼‰
    chinese_separators = [
        "ã€‚", "ï¼", "ï¼Ÿ",  # å¥æœ«æ ‡ç‚¹
        "ï¼›", "ï¼Œ", "ï¼š",  # å¥å†…æ ‡ç‚¹
        ")", "ã€‘", "}", "ã€‹",  # å³æ‹¬å·/å³ç¬¦å·
        "\n", " ", ""       # æ¢è¡Œ/ç©ºæ ¼/å…œåº•
    ]
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=chinese_separators,
        length_function=len  # ä¸­æ–‡æŒ‰å­—ç¬¦æ•°è®¡ç®—é•¿åº¦ï¼ˆè€Œéè‹±æ–‡å•è¯æ•°ï¼‰
    )
    chunks = splitter.split_documents(documents)
    
    # ä¸ºæ¯ä¸ªåˆ†å—æ·»åŠ å”¯ä¸€ IDï¼ˆé¿å…å¢é‡æ—¶é‡å¤ï¼‰
    for i, chunk in enumerate(chunks):
        src = chunk.metadata.get("source", "unknown")
        # å†…å®¹æ‘˜è¦ï¼ˆMD5 å‰ 8 ä½ï¼‰+ æ–‡ä»¶å + åºå·ï¼Œç¡®ä¿å”¯ä¸€æ€§
        content_digest = hashlib.md5(chunk.page_content.encode("utf-8")).hexdigest()[:8]
        chunk.metadata["chunk_id"] = f"{src}_{i}_{content_digest}"
        # è¡¥å……åˆ†å—åºå· metadata
        chunk.metadata["chunk_index"] = i
    
    print(f"âœ‚ï¸  åˆ†å—å®Œæˆï¼šå…± {len(chunks)} å—ï¼ˆå•å—{CHUNK_SIZE}å­—ï¼Œé‡å {CHUNK_OVERLAP}å­—ï¼‰")
    return chunks

# ========================== 4. Embedding åˆå§‹åŒ– ==========================
def init_embeddings():
    from langchain_huggingface import HuggingFaceEmbeddings

    # æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å­˜åœ¨
    model_path = str(EMBEDDING_MODEL_PATH) if EMBEDDING_MODEL_PATH.exists() else EMBEDDING_MODEL_NAME

    # è®¾å¤‡é€‰æ‹©
    device = "cuda" if torch.cuda.is_available() else "cpu"
    if device == "cuda":
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
        device = "cuda" if gpu_mem >= 4 else "cpu"

    try:
        # âœ… æ ¸å¿ƒä¿®å¤ï¼šä¸å†æ‰‹åŠ¨ä¼  clientï¼Œä¸å†ä¼  cache_folder
        # langchain-huggingface ä¼šè‡ªåŠ¨å¤„ç†ç¼“å­˜è·¯å¾„å’Œæ¨¡å‹åŠ è½½
        embeddings = HuggingFaceEmbeddings(
            model_name=model_path,
            model_kwargs={
                "device": device,
                "trust_remote_code": True,
            },
            encode_kwargs={
                "normalize_embeddings": True,           # BGE å¿…é¡»å½’ä¸€åŒ–
                "batch_size": 64,
                "prompt": "ä¸ºè¿™ä¸ªå¥å­ç”Ÿæˆè¡¨ç¤ºä»¥ç”¨äºæ£€ç´¢ç›¸å…³æ–‡ç« ï¼š"  # æ–°ç‰ˆç”¨ `prompt` æ›¿ä»£ `query_instruction`
            },
        )
        print(f"\nâœ… Embedding åŠ è½½å®Œæˆ: {model_path}ï¼ˆè®¾å¤‡ï¼š{device}ï¼‰")
        return embeddings
    except Exception as e:
        error_msg = str(e)[:150] + "..." if len(str(e)) > 150 else str(e)
        raise RuntimeError(f"Embedding åˆå§‹åŒ–å¤±è´¥: {error_msg}")

# ========================== 5. æ„å»º/åŠ è½½å‘é‡åº“ï¼ˆå«å¢é‡ + ä½™å¼¦è·ç¦»ï¼‰ ==========================
def build_vector_db(chunks, embeddings):
    from langchain_chroma import Chroma

    # æ£€æŸ¥å‘é‡åº“æ˜¯å¦å·²å­˜åœ¨
    if VECTOR_DB_DIR and os.path.exists(VECTOR_DB_DIR):
        print(f"\nğŸ” æ£€æµ‹åˆ°å·²å­˜åœ¨å‘é‡åº“ï¼š{VECTOR_DB_DIR}")
        # åŠ è½½ç°æœ‰å‘é‡åº“
        vector_db = Chroma(
            persist_directory=VECTOR_DB_DIR,
            embedding_function=embeddings,
            collection_metadata={"hnsw:space": "cosine"}  # ä½¿ç”¨ cosine ç›¸ä¼¼åº¦
        )
        # è·å–å·²å­˜åœ¨çš„åˆ†å— ID
        existing_ids = set(vector_db.get()["ids"])
        new_chunks = [c for c in chunks if c.metadata["chunk_id"] not in existing_ids]

        if new_chunks:
            print(f"ğŸ“ˆ å¢é‡æ·»åŠ  {len(new_chunks)} ä¸ªæ–°åˆ†å—...")
            vector_db.add_documents(
                documents=new_chunks,
                ids=[c.metadata["chunk_id"] for c in new_chunks]
            )
            # âœ… æ–°ç‰ˆæ— éœ€ .persist()ï¼Œæ•°æ®è‡ªåŠ¨æŒä¹…åŒ–
            total_count = len(vector_db.get()["ids"])
            print(f"âœ… å¢é‡å®Œæˆï¼šæ€»è®¡ {total_count} æ¡å‘é‡")
        else:
            total_count = len(existing_ids)
            print(f"ğŸ“Š æ— æ–°å¢å†…å®¹ï¼Œå½“å‰å…± {total_count} æ¡å‘é‡")
        return vector_db

    # æ–°å»ºå‘é‡åº“
    print(f"\nğŸ†• åˆ›å»ºæ–°å‘é‡åº“ï¼š{VECTOR_DB_DIR}")
    vector_db = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=VECTOR_DB_DIR,
        ids=[c.metadata["chunk_id"] for c in chunks],
        collection_metadata={"hnsw:space": "cosine"}  # ä½¿ç”¨ HNSW ç´¢å¼• + ä½™å¼¦è·ç¦»
    )
    print(f"âœ… æ–°å‘é‡åº“æ„å»ºå®Œæˆï¼Œå…± {len(chunks)} æ¡å‘é‡")
    return vector_db

# ========================== 6. LLM åˆå§‹åŒ–ï¼ˆ4bit ä¼˜å…ˆ + æ˜¾å­˜ä¼˜åŒ–ï¼‰ ==========================
def init_llm():
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
    from langchain_community.llms import HuggingFacePipeline
    from accelerate import Accelerator  # ä¼˜åŒ– GPU èµ„æºåˆ†é…
    
    # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜æ¨¡å‹
    model_path = str(LLM_MODEL_PATH) if LLM_MODEL_PATH.exists() else LLM_MODEL_NAME
    print(f"\nğŸ”„ æ­£åœ¨åŠ è½½ LLMï¼š{model_path}")
    
    # åˆå§‹åŒ– Tokenizerï¼ˆè§£å†³ä¸­æ–‡åˆ†è¯é—®é¢˜ï¼‰
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            use_fast=False,  # ç¦ç”¨ fast tokenizerï¼Œé¿å…ä¸­æ–‡åˆ†è¯é”™è¯¯
            padding_side="right",  # å³å¡«å……ï¼ˆé¿å…ç”Ÿæˆæ—¶è­¦å‘Šï¼‰
            cache_dir=str(MODEL_BASE_DIR.resolve())
        )
        # è¡¥å…… Tokenizer ç¼ºå¤±çš„ç‰¹æ®Š token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token  # ç”¨ eos_token ä½œä¸º pad_token
        if tokenizer.unk_token is None:
            tokenizer.unk_token = tokenizer.eos_token
        print("âœ… Tokenizer åŠ è½½å®Œæˆ")
    except Exception as e:
        raise RuntimeError(f"Tokenizer åŠ è½½å¤±è´¥: {str(e)[:100]}")
    
    # 4bit é‡åŒ–é…ç½®ï¼ˆæ˜¾å­˜ä¼˜åŒ–æ ¸å¿ƒï¼Œ7B æ¨¡å‹ 4bit çº¦å  5~6GB æ˜¾å­˜ï¼‰
    model_kwargs = {
        "torch_dtype": torch.float16,  # åŠç²¾åº¦ï¼Œå¹³è¡¡ç²¾åº¦ä¸æ˜¾å­˜
        "device_map": "auto",          # è‡ªåŠ¨åˆ†é…è®¾å¤‡ï¼ˆGPU ä¼˜å…ˆï¼Œä¸è¶³åˆ™ç”¨ CPUï¼‰
        "trust_remote_code": True,
        "low_cpu_mem_usage": True,     # ä½ CPU å†…å­˜å ç”¨
        "cache_dir": str(MODEL_BASE_DIR.resolve()),
        # 4bit é‡åŒ–å‚æ•°ï¼ˆbitsandbytes å®ç°ï¼‰
        "load_in_4bit": True,
        "bnb_4bit_quant_type": "nf4",  # å½’ä¸€åŒ–æµ®ç‚¹ï¼ˆæ›´é€‚åˆè¯­ä¹‰ä»»åŠ¡ï¼‰
        "bnb_4bit_compute_dtype": torch.float16,
        "bnb_4bit_use_double_quant": True  # åŒé‡é‡åŒ–ï¼ˆè¿›ä¸€æ­¥å‡å°‘æ˜¾å­˜ï¼‰
    }
    
    # CPU ç¯å¢ƒä¸æ”¯æŒ 4bitï¼Œç§»é™¤é‡åŒ–å‚æ•°
    if not torch.cuda.is_available():
        for k in ["load_in_4bit", "bnb_4bit_quant_type", "bnb_4bit_compute_dtype", "bnb_4bit_use_double_quant"]:
            model_kwargs.pop(k, None)
        model_kwargs["torch_dtype"] = torch.float32  # CPU ç”¨å•ç²¾åº¦
        print("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU è¿è¡Œ LLMï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®ç”¨ 3B æ¨¡å‹ï¼‰")
    
    # æ„å»º LLM Pipeline çš„å·¥å…·å‡½æ•°
    def _build_llm_pipeline(_model):
        text_gen_pipeline = pipeline(
            "text-generation",
            model=_model,
            tokenizer=tokenizer,
            max_new_tokens=512,  # ç”Ÿæˆå›ç­”çš„æœ€å¤§é•¿åº¦
            temperature=0.3,     # éšæœºæ€§ï¼ˆ0.1~0.5 é€‚åˆé—®ç­”ï¼Œè¶Šä½è¶Šä¸¥è°¨ï¼‰
            top_p=0.9,           # é‡‡æ ·é˜ˆå€¼ï¼ˆè¿‡æ»¤ä½æ¦‚ç‡è¯ï¼‰
            repetition_penalty=1.15,  # é‡å¤æƒ©ç½šï¼ˆé¿å…ç”Ÿæˆé‡å¤å†…å®¹ï¼‰
            return_full_text=False,   # ä»…è¿”å›ç”Ÿæˆçš„å›ç­”ï¼ˆä¸åŒ…å« promptï¼‰
            do_sample=True,           # é‡‡æ ·ç”Ÿæˆï¼ˆæå‡å›ç­”å¤šæ ·æ€§ï¼‰
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
        # å°è£…ä¸º LangChain å…¼å®¹çš„ LLM
        return HuggingFacePipeline(pipeline=text_gen_pipeline)
    
    # å°è¯•åŠ è½½ LLMï¼ˆ4bit â†’ 8bit â†’ å…¨ç²¾åº¦ è‡ªåŠ¨é™çº§ï¼‰
    try:
        print("ğŸ”§ å°è¯• 4bit é‡åŒ–åŠ è½½ LLM...")
        model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
        quant_mode = "4bit"
    except Exception as e1:
        print(f"âš ï¸ 4bit åŠ è½½å¤±è´¥ï¼Œé™çº§ä¸º 8bitï¼š{str(e1)[:80]}")
        # 8bit é…ç½®ï¼ˆæ˜¾å­˜çº¦ 8~10GBï¼‰
        model_kwargs_8bit = model_kwargs.copy()
        model_kwargs_8bit.pop("load_in_4bit", None)
        model_kwargs_8bit.pop("bnb_4bit_quant_type", None)
        model_kwargs_8bit.pop("bnb_4bit_compute_dtype", None)
        model_kwargs_8bit.pop("bnb_4bit_use_double_quant", None)
        model_kwargs_8bit["load_in_8bit"] = True
        
        try:
            model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs_8bit)
            quant_mode = "8bit"
        except Exception as e2:
            print(f"âš ï¸ 8bit åŠ è½½å¤±è´¥ï¼Œé™çº§ä¸ºå…¨ç²¾åº¦ï¼ˆCPU/GPUï¼‰ï¼š{str(e2)[:80]}")
            # å…¨ç²¾åº¦é…ç½®ï¼ˆGPU éœ€ â‰¥14GB æ˜¾å­˜ï¼ŒCPU éœ€ â‰¥24GB å†…å­˜ï¼‰
            model_kwargs_full = model_kwargs.copy()
            for k in ["load_in_4bit", "load_in_8bit", "bnb_4bit_quant_type", "bnb_4bit_compute_dtype", "bnb_4bit_use_double_quant"]:
                model_kwargs_full.pop(k, None)
            
            try:
                model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs_full)
                quant_mode = "å…¨ç²¾åº¦"
            except Exception as e3:
                raise RuntimeError(f"                LLM åŠ è½½å¤±è´¥ï¼š{str(e3)[:100]}")
    
    # æ„å»º LLM Pipeline å¹¶è¿”å›
    llm = _build_llm_pipeline(model)
    # æ‰“å° LLM åŠ è½½çŠ¶æ€ï¼ˆè®¾å¤‡ + é‡åŒ–æ¨¡å¼ï¼‰
    device_info = model.device if hasattr(model, 'device') else "unknown"
    print(f"âœ… LLM åŠ è½½å®Œæˆï¼ˆè®¾å¤‡ï¼š{device_info}ï¼Œé‡åŒ–ï¼š{quant_mode}ï¼‰")
    return llm

# ========================== 7. RAG ä¸»æµç¨‹ ==========================
# ä¸­æ–‡ Prompt æ¨¡æ¿ï¼ˆå¼ºè°ƒâ€œä»…ç”¨ä¸Šä¸‹æ–‡å›ç­”â€ï¼Œé¿å…æ¨¡å‹æœæ’°ï¼‰
PROMPT_TEMPLATE = """ä½ æ˜¯ä¸“ä¸šçš„ä¸­æ–‡é—®ç­”åŠ©æ‰‹ï¼Œä¸¥æ ¼éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š
1. ä»…ä½¿ç”¨ã€ä¸Šä¸‹æ–‡ã€‘ä¸­çš„ä¿¡æ¯å›ç­”é—®é¢˜ï¼Œä¸æ·»åŠ ä»»ä½•å¤–éƒ¨çŸ¥è¯†æˆ–ä¸»è§‚æ¨æµ‹ï¼›
2. è‹¥ã€ä¸Šä¸‹æ–‡ã€‘ä¸­æ²¡æœ‰ä¸é—®é¢˜ç›¸å…³çš„å†…å®¹ï¼Œç›´æ¥å›ç­”ï¼šâ€œæ ¹æ®æä¾›çš„çŸ¥è¯†åº“ï¼Œæ— æ³•å›ç­”è¯¥é—®é¢˜â€ï¼›
3. å›ç­”éœ€ç®€æ´å‡†ç¡®ï¼Œç”¨ä¸­æ–‡å£è¯­åŒ–è¡¨è¾¾ï¼Œé¿å…å†—é•¿æˆ–ç”Ÿç¡¬çš„è¡¨è¿°ï¼›
4. è‹¥é—®é¢˜æ¶‰åŠå¤šä¸ªè¦ç‚¹ï¼Œåˆ†ç‚¹å›ç­”ï¼ˆç”¨æ•°å­—åºå·ï¼‰ï¼Œä½†ä¸è¦è¿‡åº¦å±•å¼€ã€‚

ã€ä¸Šä¸‹æ–‡ã€‘
{context}

ã€é—®é¢˜ã€‘
{query}

ã€å›ç­”ã€‘
"""

def rag_pipeline(query, vector_db, llm):
    from langchain.prompts import PromptTemplate
    from langchain.chains import LLMChain

    print(f"\nğŸ” æ­£åœ¨æ£€ç´¢ä¸ã€Œ{query}ã€ç›¸å…³çš„å†…å®¹ï¼ˆTop {TOP_K}ï¼Œé˜ˆå€¼ {DISTANCE_THRESHOLD}ï¼‰...")

    # æ£€ç´¢é€»è¾‘ï¼šæ”¯æŒ MMR å»å†— / æ™®é€šç›¸ä¼¼åº¦è¿‡æ»¤
    if USE_MMR:
        # MMRï¼ˆMaximal Marginal Relevanceï¼‰ï¼šå¹³è¡¡ç›¸å…³æ€§ä¸å¤šæ ·æ€§ï¼Œé¿å…é‡å¤
        retriever = vector_db.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": TOP_K,
                "fetch_k": max(20, TOP_K * 4),  # å…ˆå¬å›æ›´å¤šå€™é€‰ï¼Œå†ç­›é€‰å¤šæ ·æ€§
                "lambda_mult": 0.5  # 0.0=ä»…ç›¸å…³æ€§ï¼Œ1.0=ä»…å¤šæ ·æ€§
            }
        )
        relevant_docs = retriever.get_relevant_documents(query)
        # MMR æ— è·ç¦»åˆ†æ•°ï¼Œç”¨å ä½ç¬¦æ ‡è®°
        docs_with_score = [(doc, "MMR") for doc in relevant_docs]
    else:
        # æ™®é€šç›¸ä¼¼åº¦æ£€ç´¢ï¼šå…ˆå¬å›æ›´å¤šå€™é€‰ï¼Œå†æŒ‰è·ç¦»é˜ˆå€¼è¿‡æ»¤
        raw_docs = vector_db.similarity_search_with_score(
            query, k=max(20, TOP_K * 4)  # å…ˆå¬å› 20 æ¡ï¼ˆé¿å…æ¼æ£€ï¼‰
        )
        # æŒ‰è·ç¦»é˜ˆå€¼è¿‡æ»¤ï¼ˆä½™å¼¦è·ç¦»è¶Šå°è¶Šç›¸ä¼¼ï¼Œä»…ä¿ç•™ < DISTANCE_THRESHOLD çš„ç»“æœï¼‰
        docs_with_score = [(d, s) for (d, s) in raw_docs if s < DISTANCE_THRESHOLD][:TOP_K]

    # æ‰“å°æ£€ç´¢ç»“æœï¼ˆæ–¹ä¾¿è°ƒè¯•ï¼‰
    print("\nğŸ“„ æ£€ç´¢åˆ°çš„ç›¸å…³ç‰‡æ®µï¼š")
    if not docs_with_score:
        print("ï¼ˆæœªæ‰¾åˆ°ç¬¦åˆé˜ˆå€¼çš„ç›¸å…³ç‰‡æ®µï¼Œå»ºè®®æ£€æŸ¥é—®é¢˜è¡¨è¿°æˆ–æé«˜ DISTANCE_THRESHOLDï¼‰")
        context = ""
    else:
        for i, (doc, score) in enumerate(docs_with_score, 1):
            # æå–æ–‡æ¡£å…ƒæ•°æ®ï¼ˆæ¥æºã€åˆ†å—åºå·ï¼‰
            source = doc.metadata.get("source", "æœªçŸ¥æ–‡ä»¶")
            chunk_idx = doc.metadata.get("chunk_index", "æœªçŸ¥")
            # å†…å®¹é¢„è§ˆï¼ˆæˆªå–å‰ 120 å­—ç¬¦ï¼Œé¿å…è¾“å‡ºè¿‡é•¿ï¼‰
            content_preview = doc.page_content[:120] + "..." if len(doc.page_content) > 120 else doc.page_content
            # æ‰“å°æ ¼å¼åŒºåˆ† MMR å’Œæ™®é€šæ£€ç´¢
            if score == "MMR":
                print(f"  {i}. æ¥æºï¼š{source}ï¼ˆåˆ†å— {chunk_idx}ï¼‰ | å†…å®¹ï¼š{content_preview}")
            else:
                # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦è·ç¦» â†’ ç›¸ä¼¼åº¦ï¼š1 - è·ç¦»ï¼ŒèŒƒå›´ 0~1ï¼‰
                similarity = max(0.0, 1 - float(score))
                print(f"  {i}. ç›¸ä¼¼åº¦ï¼š{similarity:.4f} | æ¥æºï¼š{source}ï¼ˆåˆ†å— {chunk_idx}ï¼‰ | å†…å®¹ï¼š{content_preview}")
        # ç»„è£…ä¸Šä¸‹æ–‡ï¼ˆæ‹¼æ¥æ‰€æœ‰ç›¸å…³ç‰‡æ®µçš„å†…å®¹ï¼‰
        context = "\n\n".join([doc.page_content for doc, _ in docs_with_score])
        # ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ï¼ˆé¿å…è¶…è¿‡ LLM ä¸Šä¸‹æ–‡çª—å£ï¼ŒDeepSeek-7B ä¸Šä¸‹æ–‡çª—å£ä¸º 8192ï¼‰
        if len(context) > 6000:  # é¢„ç•™ 2000+ é•¿åº¦ç»™ prompt å’Œå›ç­”
            context = context[:6000] + "\n...ï¼ˆä¸Šä¸‹æ–‡è¿‡é•¿ï¼Œå·²æˆªæ–­å…³é”®éƒ¨åˆ†ï¼‰"

    # æ„å»º Prompt å¹¶ç”Ÿæˆå›ç­”
    prompt = PromptTemplate(template=PROMPT_TEMPLATE, input_variables=["context", "query"])
    chain = LLMChain(llm=llm, prompt=prompt)

    try:
        print(f"\nğŸ§  æ­£åœ¨ç”Ÿæˆå›ç­”ï¼ˆåŸºäº {len(docs_with_score)} ä¸ªç›¸å…³ç‰‡æ®µï¼‰...")
        answer = chain.run({"context": context, "query": query}).strip()
        # æ¸…ç†å›ç­”ä¸­çš„å¤šä½™ç©ºè¡Œ
        answer = "\n".join([line.strip() for line in answer.split("\n") if line.strip()])
        return answer
    except Exception as e:
        error_msg = str(e)[:160] + "..." if len(str(e)) > 160 else str(e)
        return f"âŒ å›ç­”ç”Ÿæˆå¤±è´¥ï¼š{error_msg}\nå»ºè®®æ£€æŸ¥ï¼š1) LLM æ¨¡å‹æ˜¯å¦æ­£å¸¸åŠ è½½ 2) æ˜¾å­˜/å†…å­˜æ˜¯å¦å……è¶³"

# ========================== 8. CLI ä¸»å…¥å£ï¼ˆå‹å¥½äº¤äº’ + å¼‚å¸¸å¤„ç†ï¼‰ ==========================
def main():
    # æ£€æŸ¥æ–‡æ¡£ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
    docs_path = Path(DOCS_DIR)
    docs_path.mkdir(exist_ok=True)
    print(f"ğŸ“ æ–‡æ¡£ç›®å½•ï¼š{docs_path.resolve()}ï¼ˆè¯·å°† TXT/PDF/DOCX/MD æ–‡æ¡£æ”¾å…¥è¯¥æ–‡ä»¶å¤¹ï¼‰")

    # æ­¥éª¤ 1ï¼šåŠ è½½æ–‡æ¡£
    try:
        documents = load_documents()
    except Exception as e:
        print(f"\nâŒ æ–‡æ¡£åŠ è½½å¤±è´¥ï¼š{str(e)[:100]}")
        print("ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥æ–‡æ¡£ç›®å½•æƒé™ã€æ–‡æ¡£æ ¼å¼æ˜¯å¦æ­£å¸¸ï¼ˆé¿å…åŠ å¯†/æŸåæ–‡ä»¶ï¼‰")
        return

    # æ— æ–‡æ¡£æ—¶æç¤ºé€€å‡º
    if not documents:
        print("â— æœªåŠ è½½åˆ°ä»»ä½•æ–‡æ¡£ï¼Œè¯·åœ¨ 'knowledge_base' æ–‡ä»¶å¤¹ä¸­æ”¾å…¥æœ‰æ•ˆæ–‡æ¡£åé‡è¯•")
        return

    # æ­¥éª¤ 2ï¼šæ–‡æ¡£åˆ†å—
    try:
        chunks = split_documents(documents)
    except Exception as e:
        print(f"\nâŒ æ–‡æ¡£åˆ†å—å¤±è´¥ï¼š{str(e)[:100]}")
        print("ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥æ–‡æ¡£å†…å®¹æ˜¯å¦æœ‰ç‰¹æ®Šå­—ç¬¦ï¼Œæˆ–è°ƒæ•´ CHUNK_SIZE/CHUNK_OVERLAP å‚æ•°")
        return

    # æ­¥éª¤ 3ï¼šåˆå§‹åŒ– Embeddingï¼ˆæ”¯æŒé•œåƒå›é€€ï¼‰
    try:
        embeddings = init_embeddings()
    except Exception as e:
        print(f"\nâš ï¸ Embedding åŠ è½½å¤±è´¥ï¼Œå°è¯•å›é€€åˆ° Hugging Face å®˜æ–¹æºï¼š{str(e)[:100]}")
        # å›é€€å®˜æ–¹æºï¼ˆåˆ é™¤é•œåƒç¯å¢ƒå˜é‡ï¼‰
        if "HF_ENDPOINT" in os.environ:
            del os.environ["HF_ENDPOINT"]
        try:
            embeddings = init_embeddings()
        except Exception as e2:
            print(f"\nâŒ å›é€€å®˜æ–¹æºåä»å¤±è´¥ï¼š{str(e2)[:100]}")
            print("ğŸ’¡ å»ºè®®ï¼š1) æ£€æŸ¥ç½‘ç»œæ˜¯å¦èƒ½è®¿é—® huggingface.co 2) æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ° models ç›®å½•")
            return

    # æ­¥éª¤ 4ï¼šæ„å»º/åŠ è½½å‘é‡åº“
    try:
        vector_db = build_vector_db(chunks, embeddings)
    except Exception as e:
        print(f"\nâŒ å‘é‡åº“æ„å»ºå¤±è´¥ï¼š{str(e)[:100]}")
        print("ğŸ’¡ å»ºè®®ï¼š1) åˆ é™¤ chroma_vector_db æ–‡ä»¶å¤¹åé‡è¯• 2) æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³")
        return

    # æ­¥éª¤ 5ï¼šåˆå§‹åŒ– LLM
    try:
        llm = init_llm()
    except Exception as e:
        print(f"\nâŒ LLM åŠ è½½å¤±è´¥ï¼š{str(e)[:100]}")
        print("ğŸ’¡ å»ºè®®ï¼š1) è‹¥ç”¨ GPUï¼Œç¡®ä¿æ˜¾å­˜ â‰¥8GBï¼ˆ4bitï¼‰/10GBï¼ˆ8bitï¼‰ 2) CPU ç¯å¢ƒå»ºè®®æ”¹ç”¨ 3B æ¨¡å‹ï¼ˆå¦‚ deepseek-ai/deepseek-llm-3b-chatï¼‰ 3) æ£€æŸ¥æ¨¡å‹ä¸‹è½½æ˜¯å¦å®Œæ•´")
        return

    # æ­¥éª¤ 6ï¼šå¯åŠ¨é—®ç­”äº¤äº’
    print("\n" + "=" * 60)
    print("ğŸ’¬ RAG ä¸­æ–‡æ™ºèƒ½é—®ç­”ç³»ç»Ÿå·²å°±ç»ª")
    print("ğŸ“Œ æ“ä½œè¯´æ˜ï¼šè¾“å…¥é—®é¢˜å³å¯æŸ¥è¯¢ï¼Œè¾“å…¥ 'q'/'quit'/'é€€å‡º' ç»“æŸç¨‹åº")
    print("=" * 60)

    # äº¤äº’å¾ªç¯
    while True:
        try:
            user_query = input("\nè¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š").strip()
        except (EOFError, KeyboardInterrupt):
            # æ•è· Ctrl+C/ Ctrl+D é€€å‡ºä¿¡å·
            print("\nğŸ‘‹ ç¨‹åºå·²æ­£å¸¸é€€å‡ºï¼Œæ¬¢è¿ä¸‹æ¬¡ä½¿ç”¨ï¼")
            break

        # é€€å‡ºé€»è¾‘
        if user_query.lower() in ["q", "quit", "exit", "é€€å‡º"]:
            print("ğŸ‘‹ ç¨‹åºå·²æ­£å¸¸é€€å‡ºï¼Œæ¬¢è¿ä¸‹æ¬¡ä½¿ç”¨ï¼")
            break

        # ç©ºè¾“å…¥å¤„ç†
        if not user_query:
            print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜ï¼Œä¸èƒ½ä¸ºç©ºï¼")
            continue

        # ç”Ÿæˆå›ç­”å¹¶æ‰“å°
        answer = rag_pipeline(user_query, vector_db, llm)
        print(f"\nğŸ“ å›ç­”ç»“æœï¼š\n{answer}")
        print("\n" + "-" * 40)  # åˆ†éš”çº¿ï¼Œæå‡å¯è¯»æ€§

# ç¨‹åºå…¥å£ï¼ˆæ•è·å…¨å±€å¼‚å¸¸ï¼‰
if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™ï¼š{str(e)}")
        print("\nğŸ’¡ æ’æŸ¥æ¸…å•ï¼š")
        print("1. ä¾èµ–åŒ…æ˜¯å¦å®‰è£…å®Œæ•´ï¼šæ‰§è¡Œ pip install -r requirements.txtï¼ˆè§ä¸‹æ–¹è¯´æ˜ï¼‰")
        print("2. æ¨¡å‹ä¸‹è½½æ˜¯å¦å®Œæ•´ï¼šæ£€æŸ¥ models ç›®å½•ä¸‹æ˜¯å¦æœ‰ BGE å’Œ DeepSeek æ¨¡å‹æ–‡ä»¶")
        print("3. ç¡¬ä»¶èµ„æºæ˜¯å¦å……è¶³ï¼šGPU æ˜¾å­˜ â‰¥8GBï¼ˆ4bitï¼‰/ CPU å†…å­˜ â‰¥24GBï¼ˆå…¨ç²¾åº¦ï¼‰")
        print("\nğŸ“Œ æ¨èä¾èµ–ç‰ˆæœ¬ï¼ˆrequirements.txt å†…å®¹ï¼‰ï¼š")
        print("langchain==0.2.10\nlangchain-community==0.2.10\nlangchain-huggingface==0.1.0")
        print("chromadb==0.5.17\nsentence-transformers==2.7.0\ntransformers==4.41.1")
        print("accelerate==0.30.1\nbitsandbytes==0.43.0\ntorch==2.2.2\npypdfium2==4.27.0")
        print("docx2txt==0.8\funstructured==0.14.9")


